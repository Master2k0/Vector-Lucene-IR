{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import nltk\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import natsort\n",
    "import math\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter, OrderedDict\n",
    "from timeit import default_timer as timer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dictionary = []\n",
    "for i in range(len(os.listdir('D:/TH_TVDPT/do_an/Cranfield/'))):\n",
    "    idx = i + 1\n",
    "    information_doc = []\n",
    "    file = 'D:/TH_TVDPT/do_an/Cranfield/' + str(idx) + '.txt'\n",
    "    with open(file) as f:\n",
    "        info = f.read()\n",
    "        info = re.sub('[@*!#$^&)(_+=-?.,/]',\"\",info)\n",
    "        information_doc.append(info)\n",
    "    text_dictionary.append(information_doc)\n",
    "\n",
    "#print('Text dictionary: ', len(text_dictionary), type(text_dictionary))\n",
    "\n",
    "#print(text_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:\\TH_TVDPT\\do_an\")\n",
    "Result_List = {} #chứa các file txt liên quan tới từng câu truy vấn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"D:\\TH_TVDPT\\do_an\\TEST\\RES\"\n",
    "Cranfield = os.listdir(filePath)\n",
    "Cranfield= natsort.natsorted(Cranfield)\n",
    "Cranfield_List = []\n",
    "''' Truy cập kết quả Cranfield'''\n",
    "for f in Cranfield:\n",
    "    r = open(filePath+ \"\\\\\" + f,\"r\",encoding=\"utf-8\")\n",
    "    column_1 = []\n",
    "    for row in r:\n",
    "        txt = row.split()\n",
    "        txt = txt[1]+\".txt\"\n",
    "        column_1.append(txt)\n",
    "    Cranfield_List.append(column_1)\n",
    "#print(Cranfield_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xử lý tài liệu Cranfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list word non stop words\n",
    "start_time = time.time()\n",
    "stop_words = stopwords.words('english') + list(string.punctuation) + ['\\n']\n",
    "Word_list = []\n",
    "i = 1\n",
    "for text in text_dictionary:\n",
    "    for word in word_tokenize(text[0].lower().strip()):\n",
    "        #if word not in stop_words:\n",
    "        a = []\n",
    "        a.append(word)\n",
    "        a.append(i)\n",
    "        Word_list.append(a)\n",
    "    i += 1\n",
    "#print('Word list: ', len(Word_list), type(Word_list))\n",
    "#print(Word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#stemming bằng lemmatizer\\nlemmatizer = WordNetLemmatizer()\\nlemmaList= Word_list\\ni = 0\\nfor word in Word_list:\\n    #lemmaList.append(lemmatizer.lemmatize(word))\\n    #print(word[0])\\n    lemmaList[i][0] = lemmatizer.lemmatize(word[0])\\n    i += 1\\n#print(lemmaList)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#stemming bằng lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmaList= Word_list\n",
    "i = 0\n",
    "for word in Word_list:\n",
    "    #lemmaList.append(lemmatizer.lemmatize(word))\n",
    "    #print(word[0])\n",
    "    lemmaList[i][0] = lemmatizer.lemmatize(word[0])\n",
    "    i += 1\n",
    "#print(lemmaList)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming bằng porterStemmer\n",
    "start_time = time.time()\n",
    "ps = PorterStemmer()\n",
    "ps_list = Word_list\n",
    "i = 0\n",
    "\n",
    "for word in Word_list:\n",
    "\n",
    "    #ps_list.append(ps.stem(word))\n",
    "    ps_list[i][0] = ps.stem(word[0])\n",
    "    i += 1\n",
    "#ps_list_set = []\n",
    "#ps_list_set = set(ps_list)\n",
    "#print('Ps_list: ', len(ps_list_set), type(ps_list_set))\n",
    "#print(ps_list_set)\n",
    "#print(ps_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225470\n"
     ]
    }
   ],
   "source": [
    "print(len(ps_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7a20b81d0c34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#print(type(i[0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mps_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Tính ra từ trong dic xuất hiện bao nhiêu lần trong 1 tài liệu ( posting list chưa tối ưu)\n",
    "create = []\n",
    "for i in ps_list:\n",
    "    create.append(i[0])\n",
    "create = set(create)\n",
    "ps_list_set = []\n",
    "for i in create:\n",
    "    a = []\n",
    "    a.append(i)\n",
    "    ps_list_set.append(a)\n",
    "#print(ps_list_set)\n",
    "for i in ps_list_set:\n",
    "    #print(type(i[0]))\n",
    "    b = []\n",
    "    for x in ps_list:\n",
    "        if i[0] in x:\n",
    "            b.append(x[1])\n",
    "    i.append(b)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(len(ps_list_set))\n",
    "#print(ps_list_set)# Lưu file Dictionary & PostingList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lưu file Dictionary & PostingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove old TF_dictionary.json file\n",
      "Create new TF_dictionary.json file\n"
     ]
    }
   ],
   "source": [
    "# Write file \n",
    "if os.path.exists('TF_dictionary_Non_Stop_Stemming.json') == True:\n",
    "  os.remove('TF_dictionary_Non_Stop_Stemming.json')\n",
    "  print('Remove old TF_dictionary.json file')\n",
    "with open('TF_dictionary_Non_Stop_Stemming.json', 'a+') as f:\n",
    "  json.dump(ps_list_set, f)\n",
    "  print('Create new TF_dictionary.json file')# Load File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TF_dictionary_Non_Stop_Stemming.json', 'r') as f:\n",
    "    ps_list_set = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary = {}\n",
    "for i in ps_list_set:\n",
    "    posting_list = []\n",
    "    for x in set(i[1]):\n",
    "        a = i[1].count(x)\n",
    "        b = [x,a]\n",
    "        posting_list.append(b)\n",
    "    Dictionary[i[0]] =(len(set(i[1])),posting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tính tf_idf\n",
    "Dictionary_new = {}\n",
    "idf_dict = {}\n",
    "for key in Dictionary:\n",
    "    #idf = 1/Dictionary[key][0]\n",
    "    #idf = 1400/Dictionary[key][0]\n",
    "    #idf = math.log((1400/Dictionary[key][0]) + 1)\n",
    "    #idf = abs(math.log(1/Dictionary[key][0]))\n",
    "    idf = math.log(1400/Dictionary[key][0])\n",
    "    a = []\n",
    "    for i in Dictionary[key][1][:][:]:\n",
    "        #print(i[1])\n",
    "        b = []\n",
    "        tf_idf = i[1]*idf\n",
    "        b = [i[0],tf_idf]\n",
    "        a.append(b)\n",
    "    Dictionary_new[key] =(Dictionary[key][0],idf,a)\n",
    "    idf_dict[key] = (idf)\n",
    "    #print(Dictionary[key][1][:][:][1])\n",
    "    #for i in Dictionary[key][1][:][:]:\n",
    "    #    print(i[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Dictionary_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lấy danh sách posting có chứa tf_idf\n",
    "posting = []\n",
    "for key in Dictionary_new:\n",
    "    for i in Dictionary_new[key][2][:][:]:\n",
    "        posting.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gom tf_idf của mỗi tìa liệu\n",
    "norm_dict = {}\n",
    "for i in posting:\n",
    "    if norm_dict.get(i[0]) is None:\n",
    "        norm_dict[i[0]] = ([(i[1])])\n",
    "    else:\n",
    "        norm_dict[i[0]].append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TÍnh norm của mỗi tài liệu\n",
    "norm_dict_1 = {}\n",
    "for key in norm_dict:\n",
    "    list_norm = norm_dict[key]\n",
    "    norm = 0\n",
    "    sum_ = 0\n",
    "    for i in list_norm:\n",
    "        sum_ += i**2\n",
    "    norm = math.sqrt(sum_)\n",
    "    norm_dict_1[key]=(norm)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(norm_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_Dictionary = {}\n",
    "for key in Dictionary_new:\n",
    "    a =[]\n",
    "    for i in Dictionary_new[key][2][:][:]:\n",
    "        #print(i[1])\n",
    "        chia = i[1]/norm_dict_1[i[0]]\n",
    "        b = []\n",
    "        b.append(i[0])\n",
    "        b.append(chia)\n",
    "        a.append(b)\n",
    "    result_Dictionary[key] = (Dictionary_new[key][0],Dictionary_new[key][1],a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(result_Dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xử lý câu truy vấn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Truy cập file câu hỏi'''\n",
    "querry_source = open(\"D:/TH_TVDPT/do_an/TEST/query.txt\",\"r\",encoding=\"utf-8\")\n",
    "index = 1\n",
    "querry_dict = {}\n",
    "for querry in querry_source:\n",
    "    qr_split = querry.split(\"\\t\")\n",
    "    \n",
    "    querry_dict[index] = (qr_split[1].replace('\\n',''))\n",
    "    index +=1 \n",
    "    \n",
    "#for  in range(len(Result_List)):\n",
    "#    Result_List[querry] = Result_List[querry].replace('\\n','')\n",
    "#print(querry_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Word_list_querry_stemming_Normal_Cosin = {} # tính cosin\n",
    "result_id = 1\n",
    "for key  in querry_dict:\n",
    "    \n",
    "    Word_list_querry = {}\n",
    "    Word_list_querry_stemming = {} # stemming và stop word\n",
    "    Norm_dict_querry = {} # tính norm\n",
    "    Word_list_querry_stemming_Normal ={} # chuẩn hóa\n",
    "\n",
    "    stop_words = stopwords.words('english') + list(string.punctuation) + ['\\n']\n",
    "    \n",
    "    for word in word_tokenize(querry_dict[key].lower().strip()):\n",
    "        #if word not in stop_words:\n",
    "        word = ps.stem(word)\n",
    "        if Word_list_querry.get(word) is None:\n",
    "            Word_list_querry[word] = (1)\n",
    "        else:\n",
    "            Word_list_querry[word] = (int(Word_list_querry[word])+1)\n",
    "    #print('Word list: ', len(Word_list_querry), type(Word_list_querry))\n",
    "    #print(Word_list_querry)\n",
    "    ###############################################################################\n",
    "\n",
    "    \n",
    "    for key_1 in Word_list_querry:\n",
    "        if idf_dict.get(key_1) != None:\n",
    "            Word_list_querry_stemming[key_1] = (Word_list_querry[key_1]*idf_dict[key_1])\n",
    "        else:\n",
    "            Word_list_querry_stemming[key_1] = (0)\n",
    "            \n",
    "    #print(Word_list_querry_stemming)\n",
    "    ##################################################################################\n",
    "    #Chuẩn hóa\n",
    "    tong = 0\n",
    "    for key_1 in Word_list_querry_stemming:\n",
    "        tong = tong + (Word_list_querry_stemming[key_1]**2)\n",
    "    tong = math.sqrt(tong)\n",
    "    for key_1 in Word_list_querry_stemming:\n",
    "        Word_list_querry_stemming_Normal[key_1] = Word_list_querry_stemming[key_1]/tong\n",
    "    #print(Word_list_querry_stemming_Normal)\n",
    "    ##################################################################################\n",
    "    #Độ đo tương đồng\n",
    "    for key_1 in Word_list_querry_stemming_Normal:\n",
    "        if result_Dictionary.get(key_1) != None:\n",
    "            a =[]\n",
    "            for i in result_Dictionary[key_1][2][:][:]:\n",
    "                #print(i[1])\n",
    "                nhan = Word_list_querry_stemming_Normal[key_1]*i[1]\n",
    "                b = []\n",
    "                b.append(i[0])\n",
    "                b.append(nhan)\n",
    "                a.append(b)\n",
    "            a.sort(key=lambda x: x[1], reverse=True)\n",
    "            Word_list_querry_stemming_Normal_Cosin[result_id] = (a)\n",
    "        #else:\n",
    "        #    Word_list_querry_stemming_Normal_Cosin[result_id] = ([0,0])\n",
    "        \n",
    "    result_id += 1\n",
    "    #print(Word_list_querry_stemming_Normal_Cosin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(Word_list_querry_stemming_Normal_Cosin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_querry_lates = []\n",
    "for key in Word_list_querry_stemming_Normal_Cosin:\n",
    "    result_querry = {}\n",
    "    a = []\n",
    "    for i in Word_list_querry_stemming_Normal_Cosin[key]:\n",
    "        #print(i[0])\n",
    "        if result_querry.get(i[0]) != None:\n",
    "            result_querry[i[0]] = (float(result_querry[i[0]]) + float(i[1]))\n",
    "        else:\n",
    "            result_querry[i[0]] = (float(i[1]))\n",
    "    for key in result_querry:\n",
    "        a.append(key)\n",
    "    result_querry_lates.append(a)\n",
    "    #print(result_querry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(result_querry_lates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tính AP ,MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"D:\\TH_TVDPT\\do_an\\TEST\\RES\"\n",
    "Cranfield = os.listdir(filePath)\n",
    "Cranfield= natsort.natsorted(Cranfield)\n",
    "Cranfield_List = []\n",
    "''' Truy cập kết quả Cranfield'''\n",
    "for f in Cranfield:\n",
    "    r = open(filePath+ \"\\\\\" + f,\"r\",encoding=\"utf-8\")\n",
    "    column_1 = []\n",
    "    for row in r:\n",
    "        txt = row.split()\n",
    "        txt = txt[1]\n",
    "        column_1.append(int(txt))\n",
    "    Cranfield_List.append(column_1)\n",
    "#print(Cranfield_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(result_querry_lates))\n",
    "#print(type(Cranfield_List))\n",
    "\n",
    "Precision = []\n",
    "Recall = []\n",
    "for index in range(len(result_querry_lates)):\n",
    "    True_False = []\n",
    "    for cau in result_querry_lates[index]:\n",
    "        if cau in Cranfield_List[index]:\n",
    "            True_False.append(\"True\")\n",
    "        else:\n",
    "            True_False.append(\"False\")\n",
    "    Result_True = len(Cranfield_List[index])\n",
    "    n = 0 # Tổng số kết quả trả về\n",
    "    y = 1 # Tổng số kết quả trả về đúng\n",
    "    x = 0 # \n",
    "    p = []\n",
    "    r = []\n",
    "    for i in True_False:\n",
    "        n += 1\n",
    "\n",
    "        if str(i) == \"True\":\n",
    "            p.append(y/n)\n",
    "            r.append(y/Result_True)\n",
    "            y += 1\n",
    "    Precision.append(p)\n",
    "    Recall.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "Average_Precision = []\n",
    "percent = []\n",
    "#print(Precision[0].index(max(Precision[0])))\n",
    "\n",
    "\n",
    "for i in range(len(Precision)):\n",
    "    if(len(Precision[i]) != 0):\n",
    "        percent1 = 0.1 #phan tram do phu\n",
    "        #percent2 = 0.1 #phan tram do phu\n",
    "        stt = 0\n",
    "        percent11 = []\n",
    "        percent11.append(max(Precision[i]))\n",
    "        while(True):\n",
    "            max_p = max(Precision[i][stt:])\n",
    "            if Recall[i][stt] >= percent1:\n",
    "                percent11.append(max_p)\n",
    "                percent1 += 0.1\n",
    "            stt += 1\n",
    "            if stt == len(Precision[i]):\n",
    "                if len(percent11) != 11:\n",
    "                    percent11.append(0)\n",
    "                break\n",
    "        percent.append(percent11)\n",
    "for i in range(len(percent)):\n",
    "    if(str(np.average(percent[i])) != \"nan\"):\n",
    "        Average_Precision.append(np.average(percent[i]))\n",
    "    else:\n",
    "        Average_Precision.append(float(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Average Precision: 0.15290480246728094\n"
     ]
    }
   ],
   "source": [
    "print(\"mean Average Precision:\", np.average(Average_Precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
